Ein anderer Ansatz bemüht sich zu verstehen, welche Bedeutung dem zukommt, was man dann (irgendwie) diesem Informationsträger entnommen hat.
Aus diesen Betrachtungen ergeben sich vier Ebenen, unter denen der Begriff der Information heute allgemein betrachtet wird.
Andere Codebeispiele wären ein Binärcode, mit dem solche Buchstaben oder eine Gradangabe zwischen zwei Computerprogrammen fließen - oder (optisch/akustisch empfangene) Morsezeichen usw. Ohne Kenntnis des Codes kann das "nur Wahrgenommene" nicht interpretiert werden und ist empfängerbezogen keine ,Information'.
Um herauszufinden, welches er bestellt hat, braucht man ihm nur eine einzige Frage zu stellen: "Haben Sie Schnitzel bestellt?"
Lautet die Antwort "Ja", so hat er ein Schnitzel bestellt, lautet die Antwort "Nein", so hat er Spaghetti bestellt.
Jedoch ist das eine recht ineffiziente Methode: Wenn der Gast noch keine Bestellung aufgegeben hat, braucht man sehr viele Fragen, um es herauszufinden.
Auch die Wahrscheinlichkeiten spielen bei einer optimalen Fragestrategie eine Rolle: Wenn man beispielsweise weiß, dass die Hälfte aller Gäste Schweineschnitzel bestellt, so ist es sicher sinnvoll, erst einmal nach Schweineschnitzel zu fragen, bevor man den Rest der Karte durchgeht.
Um 27 verschiedene Zustände darstellen zu können, benötigt man mehrere Bits, in diesem Fall wären es fünf; man könnte damit 2 hoch 5 = 32 Zustände unterscheiden.
Berücksichtigt man diese Auftretenswahrscheinlichkeit der Zeichen im Zeichenvorrat, so kann man die Anzahl der benötigten Ja-/Nein-Entscheidungen, die zum Erkennen eines Zeichens notwendig sind, je nach Zeichen unterschiedlich groß machen.
Damit benötigt man, um ein häufig auftretendes Zeichen zu codieren, weniger Bits, als für ein selten auftretendes Zeichen.
Jedoch ist die Überführung von Syntax in Semantik selten so direkt; in der Regel wird die Information über sehr viele unterschiedliche Codes immer höherer semantischer Ebene verarbeitet: Dabei wird auf den unterschiedlichen semantischen Ebenen wiederum Informationsverarbeitung auf strukturell-syntaktischer Ebene geleistet: Die Lichtpulse, die gerade auf Ihre Netzhaut treffen, werden dort von Nervenzellen registriert (Bedeutung für die Nervenzelle), an das Gehirn weitergeleitet, in einen räumlichen Zusammenhang gebracht, als Buchstaben erkannt, zu Worten zusammengefügt.
Die Aussage, dass es warm ist (die wir nun semantisch richtig interpretiert haben; wir wissen, was diese Botschaft uns sagen will), hat echten Informationscharakter, wenn wir uns mittags um zwölf nach einer durchzechten Nacht noch halb schlaftrunken überlegen, was wir anziehen sollen, und uns die Freundin mit den Worten "es ist warm" davon abhält, in den Rollkragenpullover zu schlüpfen.
Smalltalk ist eine Art des Informationsaustausches, bei dem die offensichtlich über die Sprache ausgetauschten semantischen Informationen so gut wie keine pragmatische Information darstellen - wichtig sind hier die Körpersignale, deren Semantik (Freundlichkeit, Abneigung) wir erkennen und pragmatisch (mag er/sie mich?)
Letztlich definiert sich auch die pragmatische Ebene nicht zuletzt dadurch, dass sie selbst neue Information syntaktischer Natur schaffen muss (sonst hätte die Information keine Wirkung entfaltet).
Da es bislang keine anerkannte einheitliche Theorie der "Information" gibt, sondern lediglich unterschiedliche Modelle, steht eine eindeutige Definition des Begriffs "Information" noch nicht zur Verfügung, wenngleich auch eine nicht anerkannte Definition bereits zur formalen Beschreibung des Experimentiervorgangs führen konnte.
Einer der wesentlichen Unterschiede zwischen geisteswissenschaftlichen und naturwissenschaftlichen Modellen besteht darin, dass für die Naturwissenschaft bereits in einer Wechselwirkung subatomarer Teilchen ein Informationsaustausch gesehen wird (vgl. z.
Somit gehen sowohl die meisten geisteswissenschaftlichen Konzepte als auch das weitläufige Verständnis im täglichen Sprachgebrauch davon aus, dass Information immer eine funktionale Bedeutung hat, im Gegensatz zum naturwissenschaftlichen Verständnis, in dem weder Funktion noch Bedeutung zwingend konstitutive Eigenschaften von Information sind.
Claude Elwood Shannon (1948) konzipierte die mathematische Theorie der Information ursprünglich nicht für den Bereich menschlichen Handelns und menschlicher Kommunikation, sondern für die technische Optimierung von Übertragungskapazitäten.
Dazu codiert er seine Information nach bestimmten Prinzipien (beispielsweise als Abfolge von Nullen und Einsen nach dem oben erwähnten Prinzip) in einen Informationsträger, der Empfänger wertet diesen Informationsträger aus, denn auch er kennt den Code, und erhält dadurch die Information (siehe auch: Kommunikation).
Nicht immer ist jedoch ein menschlicher Sender vorhanden, der uns etwas mitteilen will.
In einem thermodynamisch offenen System kann Information weitergegeben werden, informationstragende Strukturen können sogar spontan entstehen.
Besonders Spin-Systeme (Spin = Drehimpuls atomarer und subatomarer Teilchen), insbesondere die sogenannten Spin-Gläser bzw. Ising-Modelle, sind sehr oft untersucht worden, nicht zuletzt wegen ihrer Relevanz für die Theorie neuronaler Netze.
Die Bedeutung der Information geht bei Shannon nur implizit in den Wahrscheinlichkeiten der verwendeten Zeichen ein, die letztlich nur unter Zuhilfenahme eines Menschen bestimmt werden könne, da nur der Mensch in der Lage sei, die Bedeutung eines Codes bewusst zu erfassen und dabei sinnvollen von nicht sinnvollem Code unterscheiden könne.
Deutlich wird dabei der jeweilige Ansatz auf den unterschiedlichen, oben geschilderten Ebenen zwischen der reinen Syntax bis zur Pragmatik, teilweise auch mit der besonderen Betonung des Transportcharakters von Information.
In einem engen Zusammenhang steht auch die (menschliche) Kommunikation: Die Kommunizierbarkeit gilt als eine wesentliche Eigenschaft von Information, und jegliche Kommunikation setzt Information voraus.